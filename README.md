# ğŸ¤– Chatbot GenAI - RegulaciÃ³n EnergÃ©tica CREG

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps**.

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre regulaciÃ³n energÃ©tica colombiana basÃ¡ndose en documentos oficiales de la **CREG (ComisiÃ³n de RegulaciÃ³n de EnergÃ­a y Gas)**. Utiliza Resoluciones CREG 2025 como fuente de conocimiento.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica bÃ¡sica
â”‚   â”œâ”€â”€ run_eval_advanced.py      â† evaluaciÃ³n con 7 criterios
â”‚   â”œâ”€â”€ dashboard_advanced.py     â† dashboard con visualizaciones
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_creg_didactico.txt
â”‚       â””â”€â”€ v2_creg_conciso.txt
â”œâ”€â”€ data/pdfs/                    â† documentos CREG (Resoluciones 076, 078, 079)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â””â”€â”€ eval_dataset_creg.json    â† dataset de evaluaciÃ³n CREG
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ QUICKSTART.md                 â† guÃ­a rÃ¡pida de instalaciÃ³n
â”œâ”€â”€ INSTRUCCIONES_INSTALACION.md â† guÃ­a completa
â”œâ”€â”€ COMPARTIR.md                  â† cÃ³mo compartir el proyecto
â”œâ”€â”€ PROGRESO_DESAFIO.md          â† documentaciÃ³n del desafÃ­o
â””â”€â”€ verify_installation.py        â† script de verificaciÃ³n
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
git clone https://github.com/hndiazg443/GenAIOps_CREG_Proyecto.git
cd GenAIOps_CREG_Proyecto
conda create -n genaiops-creg python=3.10 -y
conda activate genaiops-creg
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**

ğŸ”§ Para personalizar:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### 3. ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_asistente_creg_didactico")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

VersiÃ³n combinada con mÃ©tricas:
```bash
streamlit run app/main_interface.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset_creg.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Eval (QAEvalChain)`
- Registra resultados en **MLflow**

Para evaluaciÃ³n avanzada con 7 criterios:

```bash
python app/run_eval_advanced.py
```

EvalÃºa: correctness, relevance, coherence, toxicity, harmfulness, helpfulness, conciseness

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard_advanced.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de barras por criterio
- Radar chart multidimensional
- AnÃ¡lisis de preguntas problemÃ¡ticas
- Razonamientos de evaluaciÃ³n en espaÃ±ol
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions (Opcional)

âš ï¸ Los workflows de GitHub Actions han sido removidos temporalmente. Puedes recrearlos si necesitas CI/CD.

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas avanzadas
- ğŸ§ª Trazar todo en MLflow
- ï¿½ Visualizar resultados con dashboards interactivos
- ï¿½ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³n, financieroâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica (QA + Criteria)
- **Plotly** â€“ Visualizaciones interactivas
- **GitHub** â€“ Control de versiones

---

## ğŸ“ DesafÃ­o para estudiantes

ğŸ§© Parte 1: PersonalizaciÃ³n

1. Elige un nuevo dominio
Ejemplos: salud, educaciÃ³n, legal, bancario, ambiental, etc.

2. Reemplaza los documentos PDF
UbÃ­calos en data/pdfs/.

3. Modifica o crea tus prompts
Edita los archivos en app/prompts/.

4. Crea un conjunto de pruebas
En tests/eval_dataset_creg.json (o tu propio archivo), define preguntas y respuestas esperadas para evaluar a tu chatbot.

âœ… Parte 2: EvaluaciÃ³n AutomÃ¡tica

1. Ejecuta run_eval.py para probar tu sistema actual.
La evaluaciÃ³n bÃ¡sica usa QAEvalChain de LangChain, que devuelve una mÃ©trica binaria: correcto / incorrecto.

ğŸ”§ Parte 3: EvaluaciÃ³n Avanzada (âœ… Implementado en este proyecto)

1. Sistema de evaluaciÃ³n mejorado con LabeledCriteriaEvalChain:

    * âœ… "correctness" â€“ Â¿Es correcta la respuesta?
    * âœ… "relevance" â€“ Â¿Es relevante respecto a la pregunta?
    * âœ… "coherence" â€“ Â¿EstÃ¡ bien estructurada la respuesta?
    * âœ… "toxicity" â€“ Â¿Contiene lenguaje ofensivo o riesgoso?
    * âœ… "harmfulness" â€“ Â¿PodrÃ­a causar daÃ±o la informaciÃ³n?
    * âœ… "helpfulness" â€“ Â¿Es Ãºtil para el usuario?
    * âœ… "conciseness" â€“ Â¿Es concisa y directa?

    * Cada criterio registra:
        * Una mÃ©trica en MLflow (score)
        * Un razonamiento como artefacto en espaÃ±ol

    Ver: `app/run_eval_advanced.py`

ğŸ“Š Parte 4: Dashboard Avanzado (âœ… Implementado)

1. Dashboard completo en `app/dashboard_advanced.py` con:

    * âœ… MÃ©tricas por criterio (scores de 0-1)
    * âœ… GrÃ¡fico de barras comparativo
    * âœ… Radar chart multidimensional
    * âœ… Tabla detallada con todos los scores
    * âœ… AnÃ¡lisis de preguntas problemÃ¡ticas
    * âœ… Razonamientos del modelo en espaÃ±ol

ğŸ§ª Parte 5: AnÃ¡lisis y ReflexiÃ³n (Ver PROGRESO_DESAFIO.md)

1. Resultados del proyecto CREG:
    * 87.5% de precisiÃ³n en QA bÃ¡sica
    * 71.2% promedio en criterios avanzados
    * 100% en coherencia, sin toxicidad, sin contenido daÃ±ino
    * Ãreas de mejora: relevancia (62.5%) y concisiÃ³n (50%)

ğŸš€ Bonus (âœ… Implementado)

- âœ… 7 criterios implementados (mÃ¡s allÃ¡ de los 5 requeridos)
- âœ… Razonamientos en espaÃ±ol
- âœ… Visualizaciones interactivas con Plotly
- âœ… DocumentaciÃ³n completa para instalaciÃ³n y compartir
- âœ… Script de verificaciÃ³n automÃ¡tica

---

Â¡Listo para ser usado en clase, investigaciÃ³n o producciÃ³n educativa! ğŸš€
