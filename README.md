# ğŸ¤– Chatbot GenAI - RegulaciÃ³n EnergÃ©tica CREG

Este proyecto demuestra cÃ³mo construir, evaluar y automatizar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas prÃ¡cticas de **GenAIOps**.

---

## ğŸ§  Caso de Estudio

El chatbot responde preguntas sobre regulaciÃ³n energÃ©tica colombiana basÃ¡ndose en documentos oficiales de la **CREG (ComisiÃ³n de RegulaciÃ³n de EnergÃ­a y Gas)**. Utiliza Resoluciones CREG 2025 como fuente de conocimiento.

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ui_streamlit.py           â† interfaz simple del chatbot
â”‚   â”œâ”€â”€ main_interface.py         â† interfaz combinada con mÃ©tricas
â”‚   â”œâ”€â”€ run_eval.py               â† evaluaciÃ³n automÃ¡tica bÃ¡sica
â”‚   â”œâ”€â”€ run_eval_advanced.py      â† evaluaciÃ³n con 7 criterios
â”‚   â”œâ”€â”€ dashboard_advanced.py     â† dashboard con visualizaciones
â”‚   â”œâ”€â”€ rag_pipeline.py           â† lÃ³gica de ingestiÃ³n y RAG
â”‚   â””â”€â”€ prompts/
â”‚       â”œâ”€â”€ v1_asistente_creg_didactico.txt
â”‚       â””â”€â”€ v2_creg_conciso.txt
â”œâ”€â”€ data/pdfs/                    â† documentos CREG (Resoluciones 076, 078, 079)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_run_eval.py
â”‚   â””â”€â”€ eval_dataset_creg.json    â† dataset de evaluaciÃ³n CREG
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ QUICKSTART.md                 â† guÃ­a rÃ¡pida de instalaciÃ³n
â”œâ”€â”€ INSTRUCCIONES_INSTALACION.md â† guÃ­a completa
â”œâ”€â”€ COMPARTIR.md                  â† cÃ³mo compartir el proyecto
â”œâ”€â”€ PROGRESO_DESAFIO.md          â† documentaciÃ³n del desafÃ­o
â””â”€â”€ verify_installation.py        â† script de verificaciÃ³n
```

---

## ğŸš¦ Ciclo de vida GenAIOps aplicado

### 1. ğŸ§± PreparaciÃ³n del entorno

```bash
git clone https://github.com/hndiazg443/GenAIOps_CREG_Proyecto.git
cd GenAIOps_CREG_Proyecto
conda create -n genaiops-creg python=3.10 -y
conda activate genaiops-creg
pip install -r requirements.txt
cp .env.example .env  # Agrega tu API KEY de OpenAI
```

---

### 2. ğŸ” Ingesta y vectorizaciÃ³n de documentos

Procesa los PDFs y genera el Ã­ndice vectorial:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

Esto:
- Divide los documentos en chunks (por defecto `chunk_size=512`, `chunk_overlap=50`)
- Genera embeddings con OpenAI
- Guarda el Ã­ndice vectorial en `vectorstore/`
- Registra los parÃ¡metros en **MLflow**

ğŸ”§ Para personalizar:
```python
save_vectorstore(chunk_size=1024, chunk_overlap=100)
```

â™»ï¸ Para reutilizarlo directamente:
```python
vectordb = load_vectorstore_from_disk()
```

---

### 3. ğŸ§  ConstrucciÃ³n del pipeline RAG

```python
from app.rag_pipeline import build_chain
chain = build_chain(vectordb, prompt_version="v1_asistente_creg_didactico")
```

- Soporta mÃºltiples versiones de prompt
- Usa `ConversationalRetrievalChain` con `LangChain` + `OpenAI`

---

### 4. ğŸ’¬ InteracciÃ³n vÃ­a Streamlit

VersiÃ³n bÃ¡sica:
```bash
streamlit run app/ui_streamlit.py
```

VersiÃ³n combinada con mÃ©tricas:
```bash
streamlit run app/main_interface.py
```

---

### 5. ğŸ§ª EvaluaciÃ³n automÃ¡tica de calidad

Ejecuta:

```bash
python app/run_eval.py
```

Esto:
- Usa `tests/eval_dataset_creg.json` como ground truth
- Genera respuestas usando el RAG actual
- EvalÃºa con `LangChain Eval (QAEvalChain)`
- Registra resultados en **MLflow**

Para evaluaciÃ³n avanzada con 7 criterios:

```bash
python app/run_eval_advanced.py
```

EvalÃºa: correctness, relevance, coherence, toxicity, harmfulness, helpfulness, conciseness

---

### 6. ğŸ“ˆ VisualizaciÃ³n de resultados

Dashboard completo:

```bash
streamlit run app/dashboard_advanced.py
```

- Tabla con todas las preguntas evaluadas
- GrÃ¡ficos de barras por criterio
- Radar chart multidimensional
- AnÃ¡lisis de preguntas problemÃ¡ticas
- Razonamientos de evaluaciÃ³n en espaÃ±ol
- Filtrado por experimento MLflow

---

### 7. ğŸ” AutomatizaciÃ³n con GitHub Actions (Opcional)

âš ï¸ Los workflows de GitHub Actions han sido removidos temporalmente. Puedes recrearlos si necesitas CI/CD.

---

### 8. ğŸ§ª ValidaciÃ³n automatizada

```bash
pytest tests/test_run_eval.py
```

- EvalÃºa que el sistema tenga al menos 80% de precisiÃ³n con el dataset base

---

## ğŸ” Â¿QuÃ© puedes hacer?

- ğŸ’¬ Hacer preguntas al chatbot
- ğŸ” Evaluar diferentes estrategias de chunking y prompts
- ğŸ“Š Comparar desempeÃ±o con mÃ©tricas semÃ¡nticas avanzadas
- ğŸ§ª Trazar todo en MLflow
- ï¿½ Visualizar resultados con dashboards interactivos
- ï¿½ğŸ”„ Adaptar a otros dominios (legal, salud, educaciÃ³n, financieroâ€¦)

---

## âš™ï¸ Stack TecnolÃ³gico

- **OpenAI + LangChain** â€“ LLM + RAG
- **FAISS** â€“ Vectorstore
- **Streamlit** â€“ UI
- **MLflow** â€“ Registro de experimentos
- **LangChain Eval** â€“ EvaluaciÃ³n semÃ¡ntica (QA + Criteria)
- **Plotly** â€“ Visualizaciones interactivas
- **GitHub** â€“ Control de versiones

---

## âœ¨ CaracterÃ­sticas Implementadas

Este proyecto implementa un sistema RAG completo con evaluaciÃ³n avanzada:

### ğŸ¯ **1. Dominio Personalizado: RegulaciÃ³n EnergÃ©tica CREG**

- âœ… 3 Resoluciones CREG 2025 (076, 078, 079) - 50 pÃ¡ginas
- âœ… 2 Prompts especializados (didÃ¡ctico y conciso)
- âœ… Dataset de evaluaciÃ³n con 8 preguntas especÃ­ficas de CREG

### ğŸ“Š **2. Sistema de EvaluaciÃ³n Dual**

**EvaluaciÃ³n BÃ¡sica (`run_eval.py`):**
- âš¡ RÃ¡pida y simple
- ğŸ¯ Usa `QAEvalChain` de LangChain
- ğŸ“ˆ MÃ©trica binaria: correcto/incorrecto
- ğŸ”§ Ideal para: debugging rÃ¡pido y verificaciÃ³n

**EvaluaciÃ³n Avanzada (`run_eval_advanced.py`):**
- ğŸ”¬ Sistema completo con `LabeledCriteriaEvalChain`
- ğŸ“Š 7 criterios de evaluaciÃ³n:
  - âœ… **Correctness** â€“ PrecisiÃ³n de la respuesta
  - âœ… **Relevance** â€“ Pertinencia a la pregunta
  - âœ… **Coherence** â€“ Estructura y fluidez
  - âœ… **Toxicity** â€“ DetecciÃ³n de lenguaje ofensivo
  - âœ… **Harmfulness** â€“ IdentificaciÃ³n de contenido daÃ±ino
  - âœ… **Helpfulness** â€“ Utilidad para el usuario
  - âœ… **Conciseness** â€“ Brevedad y claridad
- ğŸ’¾ Cada criterio registra score (0-1) y razonamiento en MLflow
- ğŸ‡ªğŸ‡¸ Razonamientos generados en espaÃ±ol

### ï¿½ **3. Dashboard Avanzado Interactivo**

Dashboard completo en `app/dashboard_advanced.py`:
- ğŸ“Š **6 secciones de visualizaciÃ³n:**
  1. Resumen general con KPIs principales
  2. GrÃ¡fico de barras comparativo por criterio
  3. Radar chart multidimensional
  4. Tabla detallada con todos los scores
  5. AnÃ¡lisis de preguntas problemÃ¡ticas
  6. Razonamientos completos en espaÃ±ol
- ğŸ¨ Visualizaciones con Plotly
- ğŸ” Filtrado por experimento MLflow

### ğŸ“Š **4. Resultados del Proyecto CREG**

MÃ©tricas obtenidas:
- ğŸ“ˆ **87.5%** de precisiÃ³n en QA bÃ¡sica (7/8 correctas)
- ğŸ“Š **71.2%** promedio en criterios avanzados
- âœ… **100%** en coherence (estructura perfecta)
- âœ… **100%** sin toxicity (lenguaje apropiado)
- âœ… **100%** sin harmfulness (contenido seguro)
- ğŸ”§ Ãreas de mejora: relevancia (62.5%) y concisiÃ³n (50%)

### ï¿½ï¸ **5. Herramientas Adicionales**

- ğŸ“ `QUICKSTART.md` â€“ InstalaciÃ³n en 5 minutos
- ğŸ“š `INSTRUCCIONES_INSTALACION.md` â€“ GuÃ­a completa paso a paso
- ğŸ”„ `COMPARTIR.md` â€“ CÃ³mo compartir el proyecto
- ğŸ“Š `PROGRESO_DESAFIO.md` â€“ DocumentaciÃ³n detallada del desarrollo
- âœ… `verify_installation.py` â€“ Script de verificaciÃ³n automÃ¡tica

---

## ğŸ“ AdaptaciÃ³n a Otros Dominios

Â¿Quieres adaptar este proyecto a tu dominio? Sigue estos pasos:

1. **Reemplaza los documentos:** Coloca tus PDFs en `data/pdfs/`
2. **Crea tus prompts:** Edita archivos en `app/prompts/`
3. **Define tu dataset:** Crea preguntas/respuestas en `tests/`
4. **Regenera vectorstore:** `python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"`
5. **Ejecuta evaluaciÃ³n avanzada:** `python app/run_eval_advanced.py`
6. **Visualiza resultados:** `streamlit run app/dashboard_advanced.py`

Dominios sugeridos: salud, educaciÃ³n, legal, financiero, ambiental, tecnolÃ³gico.

---

Â¡Sistema completo de GenAIOps listo para producciÃ³n o investigaciÃ³n! ğŸš€
