"""
ğŸ‰ RESUMEN FINAL DEL DESAFÃO GENAIOPS - PYCON 2025
AnÃ¡lisis completo del sistema implementado
"""

import mlflow
import pandas as pd
from datetime import datetime

print("="*80)
print("ğŸ“ DESAFÃO ESTUDIANTE GENAIOPS - RESUMEN FINAL")
print("="*80)
print(f"ğŸ“… Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*80)

# Obtener experimentos
client = mlflow.tracking.MlflowClient()
experiments = [exp for exp in client.search_experiments() if exp.name.startswith("eval_")]

print(f"\nâœ… Total de experimentos creados: {len(experiments)}")
for exp in experiments:
    print(f"   ğŸ“Š {exp.name}")

# Analizar experimento avanzado
exp_name = "eval_advanced_v1_asistente_creg_didactico"
experiment = client.get_experiment_by_name(exp_name)

if experiment:
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ANÃLISIS DEL EXPERIMENTO PRINCIPAL: {exp_name}")
    print(f"{'='*80}")
    
    runs = client.search_runs(experiment_ids=[experiment.experiment_id])
    
    print(f"\nâœ… Total de evaluaciones: {len(runs)}")
    
    # Recopilar mÃ©tricas
    data = []
    for run in runs:
        metrics = run.data.metrics
        data.append(metrics)
    
    df = pd.DataFrame(data)
    
    # Calcular estadÃ­sticas
    print("\nğŸ“Š RESULTADOS GLOBALES:")
    print("-"*80)
    
    criteria = [
        ("âœ… QA BÃ¡sica", "qa_is_correct"),
        ("ğŸ¯ CorrecciÃ³n", "correctness_score"),
        ("ğŸ“ Relevancia", "relevance_score"),
        ("ğŸ“ Coherencia", "coherence_score"),
        ("ğŸ›¡ï¸ Toxicidad", "toxicity_score"),
        ("âš ï¸ DaÃ±o Potencial", "harmfulness_score"),
        ("ğŸ’¡ Utilidad", "helpfulness_score"),
        ("ğŸ“ ConcisiÃ³n", "conciseness_score"),
        ("ğŸ“ˆ Promedio Global", "avg_criteria_score")
    ]
    
    for label, col in criteria:
        if col in df.columns:
            avg = df[col].mean()
            emoji = "ğŸŸ¢" if avg >= 0.7 else "ğŸŸ¡" if avg >= 0.5 else "ğŸ”´"
            print(f"{emoji} {label:25s}: {avg*100:5.1f}%")
    
    print("\n" + "="*80)
    print("ğŸ† LOGROS DESTACADOS:")
    print("="*80)
    
    if 'toxicity_score' in df.columns:
        tox_avg = df['toxicity_score'].mean()
        if tox_avg == 1.0:
            print("ğŸ›¡ï¸  Â¡CERO TOXICIDAD! - Todas las respuestas son respetuosas")
    
    if 'harmfulness_score' in df.columns:
        harm_avg = df['harmfulness_score'].mean()
        if harm_avg == 1.0:
            print("âœ… Â¡CERO CONTENIDO DAÃ‘INO! - InformaciÃ³n segura al 100%")
    
    if 'coherence_score' in df.columns:
        coh_avg = df['coherence_score'].mean()
        if coh_avg == 1.0:
            print("ğŸ“ Â¡COHERENCIA PERFECTA! - Estructura impecable en todas las respuestas")
    
    if 'qa_is_correct' in df.columns:
        qa_avg = df['qa_is_correct'].mean()
        correct_count = df['qa_is_correct'].sum()
        total = len(df[df['qa_is_correct'].notna()])
        print(f"âœ… PRECISIÃ“N: {correct_count:.0f}/{total} preguntas correctas ({qa_avg*100:.1f}%)")
    
    print("\n" + "="*80)
    print("âš ï¸ ÃREAS DE MEJORA IDENTIFICADAS:")
    print("="*80)
    
    if 'conciseness_score' in df.columns:
        conc_avg = df['conciseness_score'].mean()
        if conc_avg < 0.5:
            print(f"ğŸ“ CONCISIÃ“N: {conc_avg*100:.1f}% - Sistema muy verboso")
            print("   ğŸ’¡ SoluciÃ³n: Usar prompt v2_creg_conciso.txt")
    
    if 'correctness_score' in df.columns:
        corr_avg = df['correctness_score'].mean()
        if corr_avg < 0.7:
            print(f"ğŸ¯ CORRECCIÃ“N: {corr_avg*100:.1f}% - Mejorable")
            print("   ğŸ’¡ SoluciÃ³n: Ajustar chunk_size o mejorar retrieval")

print("\n" + "="*80)
print("ğŸ“š ARCHIVOS CREADOS EN EL PROYECTO:")
print("="*80)

files_created = [
    ("ğŸ“„ Prompts", [
        "app/prompts/v1_asistente_creg_didactico.txt",
        "app/prompts/v2_creg_conciso.txt"
    ]),
    ("ğŸ“Š EvaluaciÃ³n", [
        "app/run_eval_advanced.py",
        "app/view_mlflow_results.py",
        "tests/eval_dataset_creg.json"
    ]),
    ("ğŸ“ˆ VisualizaciÃ³n", [
        "app/dashboard_advanced.py"
    ]),
    ("ğŸ“ DocumentaciÃ³n", [
        "PROGRESO_DESAFIO.md"
    ])
]

for category, files in files_created:
    print(f"\n{category}:")
    for file in files:
        print(f"   âœ… {file}")

print("\n" + "="*80)
print("ğŸš€ COMANDOS ÃšTILES:")
print("="*80)
print("\n# Ver evaluaciÃ³n avanzada:")
print("python app/run_eval_advanced.py")
print("\n# Ver resumen de resultados:")
print("python app/view_mlflow_results.py")
print("\n# Abrir dashboard avanzado:")
print("streamlit run app/dashboard_advanced.py")
print("\n# Abrir MLflow UI:")
print("mlflow ui")
print("# Luego: http://localhost:5000")

print("\n" + "="*80)
print("âœ… DESAFÃO COMPLETADO AL 100%")
print("="*80)
print("ğŸ“Š Sistema de evaluaciÃ³n multidimensional funcional")
print("ğŸ¯ 7 criterios de evaluaciÃ³n implementados")
print("ğŸ“ˆ Dashboard interactivo con visualizaciones avanzadas")
print("ğŸ’¾ Resultados y razonamientos guardados en MLflow")
print("="*80)
print("\nğŸ‰ Â¡FELICITACIONES! Has completado exitosamente el desafÃ­o GenAIOps")
print("="*80)
